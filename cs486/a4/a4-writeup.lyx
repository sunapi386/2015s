#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\begin_preamble
\usepackage{graphviz}

\usepackage{lastpage}
\usepackage[ddmmyyyy]{datetime}

\fancyhf{} % clear existing head/footers
\fancyhead[L]{CS 486}
\fancyhead[C]{Assignment 4}
\fancyhead[R]{j53sun \#20387090}

\fancyfoot[L]{\fontsize{8}{8} \today: \currenttime}
\fancyfoot[R]{\fontsize{8}{8} \thepage\ / \pageref{LastPage}}

\makeatletter
\let\ps@plain\ps@fancy   % Plain page style = fancy page style
\makeatother
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 1
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize letterpaper
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\topmargin 3cm
\bottommargin 3cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle fancy
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
CS 486 Assignment 4
\end_layout

\begin_layout Subsection*
1.
 Proving sigmoid function properties
\end_layout

\begin_layout Standard
Where 
\begin_inset Formula $\sigma(a)=\frac{1}{1+e^{-a}}$
\end_inset

.
 Note alternative representations 
\begin_inset Formula $\sigma(a)=\frac{e^{a}}{1+e^{a}}=1-\frac{1}{1+e^{a}}.$
\end_inset


\end_layout

\begin_layout Standard
Prove 
\begin_inset Formula $\sigma(-a)=1-\sigma(a)$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\sigma(-a) & = & 1-\sigma(a)\\
 & = & 1-\frac{1}{1+e^{-a}}\\
 & = & \frac{1+e^{-a}}{1+e^{-a}}-\frac{1}{1+e^{-a}}\\
 & = & \frac{e^{-a}}{1+e^{-a}}\\
 & = & \frac{1}{1+e^{a}}\\
 & = & \frac{1}{1+e^{-(-a)}}\\
 & = & \sigma(-a)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Prove 
\begin_inset Formula $\sigma^{-1}(a)=\ln(\frac{a}{1-a})$
\end_inset

.
 Let 
\begin_inset Formula $y=\sigma$
\end_inset

, solve for 
\begin_inset Formula $a$
\end_inset

 and switch the 
\begin_inset Formula $a$
\end_inset

 and 
\begin_inset Formula $y$
\end_inset

 back; the new 
\begin_inset Formula $y$
\end_inset

 is the inverse 
\begin_inset Formula $\sigma^{-1}$
\end_inset

 function.
 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
y & = & \frac{1}{1+e^{-a}}\\
y+ye^{-a} & = & 1\\
e^{-a} & = & \frac{1-y}{y}\\
\ln(e^{-a}) & = & \ln(\frac{1-y}{y})\\
a & = & -\ln(\frac{1-y}{y})\\
a & = & \ln(\frac{y}{1-y})\\
y & = & \ln(\frac{a}{1-a})\\
\sigma^{-1}(a) & = & \ln(\frac{a}{1-a})
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Prove 
\begin_inset Formula $\frac{\partial\sigma}{\partial a}=\sigma(a)(1-\sigma(a))$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial\sigma}{\partial a} & = & \sigma(a)(1-\sigma(a))\\
 & = & \sigma(a)-\sigma^{2}(a)\\
 & = & \frac{e^{a}}{e^{a}+1}-(\frac{e^{a}}{e^{a}+1})^{2}\\
 & = & \frac{e^{a}(e^{a}+1)}{(e^{a}+1)^{2}}-\frac{e^{2a}}{(e^{a}+1)^{2}}\\
 & = & \frac{e^{2a}+e^{a}-e^{2a}}{(e^{a}+1)^{2}}\\
 & = & \frac{e^{a}}{(e^{a}+1)^{2}}\\
 & = & \frac{\partial}{\partial a}(\frac{1}{1+e^{-a}})\\
 & = & \frac{\partial\sigma}{\partial a}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsection*
2.
 Neural network using 
\begin_inset Formula $\tanh$
\end_inset

 instead of 
\begin_inset Formula $\sigma$
\end_inset


\end_layout

\begin_layout Standard
For 
\begin_inset Formula $\tanh$
\end_inset

 and 
\begin_inset Formula $\sigma$
\end_inset

, their relation is a scaling plus a linear transformation.
 
\begin_inset Formula $\tanh(a)=2\sigma(2a)-1$
\end_inset

.
 Subsituiting in definition of 
\begin_inset Formula $\sigma$
\end_inset

 proves this.
 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\tanh(a) & = & 2\sigma(2a)-1\\
 & = & 2(\frac{1}{1+e^{-2a}})-1\\
 & = & -\frac{e^{-2a}-1}{e^{-2a}+1}\\
 & = & \frac{1-e^{-2a}}{1+e^{-2a}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
This is (one of) the definition of 
\begin_inset Formula $tanh$
\end_inset

.
 Isolating for 
\begin_inset Formula $\sigma$
\end_inset

, 
\begin_inset Formula $\sigma(a)=\frac{\tanh(a/2)+1}{2}$
\end_inset

.
 So replacing the activation function 
\begin_inset Formula $g(\cdot)$
\end_inset

 from using 
\begin_inset Formula $\sigma$
\end_inset

 to using 
\begin_inset Formula $tanh$
\end_inset

 can be done by applying this transformation.
 
\end_layout

\begin_layout Standard
Originally, as given in assignment, 
\begin_inset Formula $g=\sigma$
\end_inset

: 
\begin_inset Formula 
\[
y_{i}(x,W)=\sigma(\sum_{j}W_{ji}^{(2)}g(\sum_{k}W_{kj}^{(1)}x_{k}+W_{0j}^{(1)})+W_{0i}^{(2)})
\]

\end_inset


\end_layout

\begin_layout Standard
Note the replacement of 
\begin_inset Formula $g(\sum_{k}W_{kj}^{(1)}x_{k}+W_{0j}^{(1)})$
\end_inset

 when 
\begin_inset Formula $g=\sigma$
\end_inset

 with 
\begin_inset Formula $\frac{g(\frac{\sum_{k}W_{kj}^{(1)}x_{k}+W_{0j}^{(1)}}{2})+1}{2}$
\end_inset

 when 
\begin_inset Formula $g=\tanh$
\end_inset

.
 
\end_layout

\begin_layout Standard
Using 
\begin_inset Formula $g=\tanh$
\end_inset

, the equivalent network is:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y_{i}(x,W)=\sigma(\sum_{j}W_{ji}^{(2)}(\frac{\tanh(\frac{\sum_{k}W_{kj}^{(1)}x_{k}+W_{0j}^{(1)}}{2})+1}{2})+W_{0i}^{(2)})
\]

\end_inset


\end_layout

\begin_layout Standard
This can be rewritten in form given earlier.
 Let new vector 
\begin_inset Formula $V$
\end_inset

 be a linear transformation of 
\begin_inset Formula $W$
\end_inset

.
 
\begin_inset Formula 
\[
y_{i}(x,W)=\sigma(\sum_{j}V_{ji}^{(2)}\tanh(\sum_{k}V_{kj}^{(1)}x_{k}+V_{0j}^{(1)})+V_{0i}^{(2)})
\]

\end_inset


\end_layout

\begin_layout Standard
Where 
\begin_inset Formula $V=2W$
\end_inset

.x
\end_layout

\begin_layout Subsection*
3.
 Threshold perceptron learning algorithm
\end_layout

\begin_layout Standard
A network with all the inputs connected directly to the outputs is called
 a single-layer neural network, or a perceptron network.
 The first thing to notice is that a perceptron network with m outputs is
 really m separate networks, because each weight affects only one of the
 outputs.
 Thus, there will be m sepa- rate training processes.
\end_layout

\begin_layout Standard
The activation function g is typically either a hard threshold (Figure 18.17(a)),
 in which case the unit is called a perceptron, or a logistic function (Figure
 18.17(b)), in which case the term sigmoid perceptron is sometimes used.
\end_layout

\begin_layout Subsubsection*
Is the dataset linearly separable?
\end_layout

\begin_layout Standard
The data is linearly seperable because training converged to weights that
 classified all of the training data correctly.
 If it 
\emph on
wasn't
\emph default
 linearly seperable then the perceptron cannot learn it.
\end_layout

\begin_layout Subsubsection*
Train and test accuracy
\end_layout

\begin_layout Standard
Trained correctly in 75 iterations Correct predictions: 93.9394% (341/363).
\end_layout

\begin_layout Subsubsection*
Final weights of the threshold perceptron
\end_layout

\begin_layout Standard
There are 65 total attributes: 64 image attributes plus 1 constant.
\end_layout

\begin_layout Standard
Final weights: -31 104 155 -242 -107 -56 -360 -119 80 158 -170 -115 24 -293
 -505 -294 -87 60 -165 -137 -81 -166 -5 -188 -54 -220 -62 80 -184 -154 -276
 16 -225 98 -113 90 18 191 50 -87 50 120 17 658 307 194 -87 401 -170 -321
 -224 156 498 -148 192 238 49 -117 -327 -230 -180 183 393 53 90
\end_layout

\begin_layout Subsubsection*
Matlab source code
\end_layout

\begin_layout Standard
\begin_inset CommandInset include
LatexCommand verbatiminput
filename "src/question3.m"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset include
LatexCommand verbatiminput
filename "src/percept_threshold.m"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset include
LatexCommand verbatiminput
filename "src/trained_correctly.m"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset include
LatexCommand verbatiminput
filename "src/predict_using_weights.m"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset include
LatexCommand verbatiminput
filename "src/f_step.m"

\end_inset


\end_layout

\begin_layout Subsection*
4.
 Feed forward neural network
\end_layout

\begin_layout Subsubsection*
Graph of train and test accuracy as function of number of hidden nodes (5
 to 15)
\end_layout

\begin_layout Subsubsection*
Discussion of results in the graph
\end_layout

\begin_layout Subsubsection*
Which algorithm (threshold perceptron vs neural network) performs best
\end_layout

\begin_layout Standard
The logistic activation function has the advantage of being differentiable.
 
\end_layout

\begin_layout Subsubsection*
Source code
\end_layout

\end_body
\end_document
