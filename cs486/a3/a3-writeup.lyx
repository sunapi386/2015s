#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\begin_preamble
\usepackage{graphviz}

\usepackage{lastpage}
\usepackage[ddmmyyyy]{datetime}

\fancyhf{} % clear existing head/footers
\fancyhead[L]{CS 486}
\fancyhead[C]{Assignment 3}
\fancyhead[R]{j53sun \#20387090}

\fancyfoot[L]{\fontsize{8}{8} \today: \currenttime}
\fancyfoot[R]{\fontsize{8}{8} \thepage\ / \pageref{LastPage}}

\makeatletter
\let\ps@plain\ps@fancy   % Plain page style = fancy page style
\makeatother
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 1
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize letterpaper
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\topmargin 3cm
\bottommargin 3cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle fancy
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
CS 486 Assignment 3
\end_layout

\begin_layout Author
Jason Sun (#20387090)
\end_layout

\begin_layout Section*
Question 1: Decision Tree Learning
\end_layout

\begin_layout Subsection*
Source Code
\end_layout

\begin_layout Standard
Written in python.
\end_layout

\begin_layout Standard
\begin_inset CommandInset include
LatexCommand verbatiminput
filename "q1_decision_tree.py"
lstparams "language=Python"

\end_inset


\end_layout

\begin_layout Subsection*
Overfitting
\end_layout

\begin_layout Subsection*
\begin_inset Graphics
	filename accuracy.png
	scale 70

\end_inset


\end_layout

\begin_layout Standard
Overfitting occurs when the accuracy of the test data has gone past its
 apex.
 At depth of 22 to 25 it reaches the maximal accuracy of 84.3%, and afterward
 there is a noticable a drop in accuracy and from there onward, it is never
 gets as good as before.
\end_layout

\begin_layout Subsection*
Highest Accuracy Decision Tree
\end_layout

\begin_layout Standard
I apologize for this poor representation.
 It is generated by the program.
 I tried to generate a nice looking 
\family typewriter
.dot
\family default
 file and compile it to a nice image, but it is too time consuming this
 time.
\end_layout

\begin_layout Itemize
The graph sort of needs to be viewed sideways.
 
\end_layout

\begin_layout Itemize
The number indicates which depth the node is at.
 
\end_layout

\begin_layout Itemize
Each node may have up to two children, left and right (representing negative
 and positive, respectively).
 
\end_layout

\begin_deeper
\begin_layout Itemize
For example, the 0th depth with the word 
\begin_inset Quotes eld
\end_inset

writes
\begin_inset Quotes erd
\end_inset

 has 2 children of depth 1 (called 
\begin_inset Quotes eld
\end_inset

god
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

graphics
\begin_inset Quotes erd
\end_inset

).
 
\end_layout

\end_deeper
\begin_layout Itemize
This is an in-order traversal, and positives are printed first.
 
\end_layout

\begin_deeper
\begin_layout Itemize
So to right of a node (child above the parent) is a decision that categories
 yes.
\end_layout

\begin_layout Itemize
And to the left of a node (child below the parent) is a decision that categories
 no.
\end_layout

\begin_layout Itemize
For example, the 0th depth parent has positive of graphics and negative
 of god.
 
\end_layout

\end_deeper
\begin_layout Itemize
Positive categorizes the computer graphics topic (because we like graphics
 more!).
 Negative categorizes the atheism topic.
\end_layout

\begin_layout Standard
\begin_inset CommandInset include
LatexCommand verbatiminput
filename "decision_tree.txt"

\end_inset


\end_layout

\begin_layout Subsection*
Features Used
\end_layout

\begin_layout Standard
The features used for the highest accuracy are:
\end_layout

\begin_layout Standard
\begin_inset CommandInset include
LatexCommand verbatiminput
filename "features_used.txt"

\end_inset


\end_layout

\begin_layout Standard
In my opinion these selected words makes sense.
 For instance, the top two most important words 
\begin_inset Quotes eld
\end_inset

god
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

graphics
\begin_inset Quotes erd
\end_inset

 has a strong utility, meaning if I see those words in an article, I'm already
 fairly sure of which topic the document is from.
 Of course, the lesser important words still make sense, but not as obvious.
 So in the inductive sense, it all makes sense, just less and less so to
 the human reader.
\end_layout

\begin_layout Section*
Question 2: Naive Bayes Model
\end_layout

\begin_layout Subsection*
Source Code
\end_layout

\begin_layout Standard
Written in matlab.
\end_layout

\begin_layout Standard
Function that generates the bayes net:
\end_layout

\begin_layout Standard
\begin_inset CommandInset include
LatexCommand verbatiminput
filename "naive_bayes_net.m"

\end_inset


\end_layout

\begin_layout Standard
The main program:
\end_layout

\begin_layout Standard
\begin_inset CommandInset include
LatexCommand verbatiminput
filename "naive_bayes.m"

\end_inset


\end_layout

\begin_layout Subsection*
Results
\end_layout

\begin_layout Standard
Top 10 most discriminative words: 
\end_layout

\begin_layout Verbatim
'4.4242' 'graphics' 
\end_layout

\begin_layout Verbatim
'3.9752' 'atheism' 
\end_layout

\begin_layout Verbatim
'3.9286' 'religion' 
\end_layout

\begin_layout Verbatim
'3.8545' 'keith' 
\end_layout

\begin_layout Verbatim
'3.8545' 'moral' 
\end_layout

\begin_layout Verbatim
'3.8545' 'evidence' 
\end_layout

\begin_layout Verbatim
'3.8286' 'atheists' 
\end_layout

\begin_layout Verbatim
'3.7837' 'god' 
\end_layout

\begin_layout Verbatim
'3.7463' 'bible' 
\end_layout

\begin_layout Verbatim
'3.7173' 'christian'
\end_layout

\begin_layout Standard
These seem like good word features.
 I can classify each of these words into the athesim or the graphics category.
 The only word that puzzles me is 
\begin_inset Quotes eld
\end_inset

keith
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Subsection*
Training and test accuracy
\end_layout

\begin_layout Standard
Classified 77.2856% correct for training data.
 Classified 0.72136% correct for testing data.
\end_layout

\begin_layout Subsection*
Independence assumption
\end_layout

\begin_layout Standard
The assumption that word features are independent is not a reasonable assumption.
 Same or similar words to what was used before are more likely to show up
 again in the future because when writing involves context.
\end_layout

\begin_layout Standard
For example, translation of documents to a different language is a difficult
 process, because the choice of words depends on context, especially in
 cases where multiple words have similar meanings.
\end_layout

\begin_layout Subsection*
Improvements to the Naive Bayes model
\end_layout

\begin_layout Standard
The improvement is difficult without changing too much the modeling method.
 Introducing a dependency of words to the model would change it from being
 a naive Bayes model.
 Nevertheless, few improvements can be made, as discussed in an MIT paper.
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
http://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The improvements can be generalized to be:
\end_layout

\begin_layout Enumerate
Analysing term frequency
\end_layout

\begin_layout Enumerate
Analysing document frequency
\end_layout

\begin_layout Enumerate
Analysing document length frequency
\end_layout

\begin_layout Subsection*
Decision tree model vs Naive Bayes model
\end_layout

\begin_layout Standard
The decision tree modeling approach works better because it can model importance
 of words based in a hierarchical fashion.
 If a word is close to the root node, then it will have a greater effect
 on the decision.
 For instance, the keyword 'graphics' showed up, then we probably do not
 need to look at any more words in the document before making a decision.
 This means word ordering matters, so a document containing just "god graphics"
 is classified as atheism topic, which is differently than "graphics god",
 classified as graphics topic.
 Whereas the naive Bayes model gives every word an equal weighting, so that
 "god graphics" is the same category as "graphics god", both being in the
 graphics topic.
\end_layout

\end_body
\end_document
