CS 486 Assignment 3
====================
Jason Sun (#20387090)

Question 2: Naive Bayes Model
-----------------------------

### Code printout

    loadScript

    % split training set into two documents
    doc1 = [];
    doc2 = [];
    for i = 1:length(trainLabel)
       if trainLabel(i) == 1
           doc1 = [doc1; trainDataSparse(i,:)];
       else
           doc2 = [doc2; trainDataSparse(i,:)];
       end
    end

    % calc Pr(word|label1) and Pr(word|label2)
    word_likelihood1 = zeros(length(trainDataSparse(1,:)),1);
    word_likelihood2 = zeros(length(trainDataSparse(1,:)),1);

    for word = 1:length(trainDataSparse(1,:))
       count = 1;
       for i = 1:size(doc1, 1)
          count = count + doc1(i,word);
       end
       word_likelihood1(word) = count / size(doc1, 1);

       count = 2;
       for i = 1:size(doc2, 1)
          count = count + doc2(i,word);
       end
       word_likelihood2(word) = count / size(doc2, 1);

    end

    log_likelihood = abs(log(word_likelihood1) - log(word_likelihood2));

    % list the N most discrimative word features
    % get the indices of the N largest elements in a matrix
    [sortedVals, sortedIdx] = sort(log_likelihood(:), 'descend');
    N = 10;
    maxIdxs = sortedIdx(1:N);
    disp(['Top ', num2str(N), ' words are:']);
    disp(words(maxIdxs))

### Results

    Top 10 most discriminative words:
    '4.4242'    'graphics'
    '3.9752'    'atheism'
    '3.9286'    'religion'
    '3.8545'    'keith'
    '3.8545'    'moral'
    '3.8545'    'evidence'
    '3.8286'    'atheists'
    '3.7837'    'god'
    '3.7463'    'bible'
    '3.7173'    'christian'

These seem like good word features. I can classify each of these words into
the athesim or the graphics category. The only word that puzzles me is 'keith'.

### Training and test accuracy

Classified 77.2856% correct for training data.
Classified 0.72136% correct for testing data.

### Independence assumtion

The assumption that word features are independent is not a reasonable assumption.
Same or similar words to what was used before are more likely to show up again in
the future because when writing involves context.

For example, translation of documents to a different language is a difficult
process, because the choice of words depends on context, especially in cases
where multiple words have similar meanings.

### Improvements to the naive Bayes model

The improvement is difficult without changing too much the modeling method.
Introducing a dependency of words to the model would change it from being a
naive Bayes model. Nevertheless, few improvements can be made, as discussed
in an MIT paper.
[http://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf]

The improvements can be generalized to be:

1.  Analysing term frequency
2.  Analysing document frequency
3.  Analysing document length frequency


### Decision tree model vs naive Bayes model

The decision tree modeling approach works better because it can model
importance of words based in a hierarchical fashion. If a word is close to the
root node, then it will have a greater effect on the decision. For instance,
the keyword 'graphics' showed up, then we probably do not need to look at any
more words in the document before making a decision. This means word ordering
matters, so a document containing just "god graphics" is classified as atheism
topic, which is differently than "graphics god", classified as graphics topic.
Whereas the naive Bayes model gives every word an equal weighting, so that
"god graphics" is the same category as "graphics god", both being in the
graphics topic.


