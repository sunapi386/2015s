{\rtf1\ansi\ansicpg1252\cocoartf1348\cocoasubrtf170
{\fonttbl\f0\fnil\fcharset0 Menlo-Regular;}
{\colortbl;\red255\green255\blue255;}
\margl1440\margr1440\vieww15840\viewh8060\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural

\f0\fs18 \cf0 Is it in our self-interest to being g   Is it in our self-interest to being g\
word count: 1834                      | word count: 1953\
]                                       ]\
\
#20387090                               #20387090\
\
July 27, 2015                         | June 29, 2015\
\
It is in our self-interest to be good | We explore this problem from the pers\
through two parts. First, being good  | agent would act, and their problem so\
rewards. Second, being good benefits  | follow up, the diligent reader would \
individually.                         | many different methods to solving a p\
                                      > methods would involve being good, and\
                                      > when it is possible to achieve the sa\
                                      > unjust means? In response to this, we\
                                      > an example of an unjust man versus a \
                                      > easier to be good.\
\
The main arguments are drawn from two | The main arguments are drawn from the\
textbook,  The Fundamentals of Ethics <\
Shafer-Landau, R. (2014). The Fundame   Shafer-Landau, R. (2014). The Fundame\
University Press                        University Press\
], and the textbook used in CS 486 In | ], with secondary arguments from an a\
Intelligence, called Artificial Intel | textbook[footnote:\
Russell, S.J. & Norvig, P. (2010). Ar   Russell, S.J. & Norvig, P. (2010). Ar\
Modern Approach. Prentice Hall          Modern Approach. Prentice Hall\
] (referred as AIMA). There are simil | ], the readings \'93The Ring of Gyges\'94 a\
fields, different approaches, but sam | Problems\'94.\
\
As in the case with mathematical proo |   Definition of our. \
definitions to the thesis.            <\
\
  Definition of our                   | We define the term our in the questio\
                                      | interest to be good?\'94 to mean a ratio\
The term our refers to a rational age | a fundamental assumption we make, in \
that always acts to achieve the best  | intelligence is plagued with emotiona\
uncertainty, the best expected outcom | irrational decisions. Hence it is in \
Definition from page 4 of AIMA        | acting upon emotional decisions, and \
]. This simplifying assumption is mad | rational. We also define good later o\
results in irrational decisions.      | framework.\
                                      |\
The philosophy text defines rationali |   From a Rational Agent's Perspective\
theory) slightly differently. Rationa |\
on one's preference orderings.        | First, let's look at the reasons why \
                                      | agent's self-interest to be good, in \
Both are similar, but I highlight tha | the future, and secondly, why it bene\
includes acting under uncertainty. Th | from being good.\
                                      |\
  Definition of good                  |   Definition of good.\
                                      |\
Being good means having insight and l | We're now ready to define what good m\
term. Consequently, as I will explore | self-interest to be good\'94: being good\
selfish (which is always trying to ma | always maximizing the reward for your\
yourself).                            | necessarily picking the most seemingl\
                                      | moment.\
  Part I: Long term rewards           |\
                                      |   Firstly, why it would be in a ratio\
The key insight into gaining more lon |   be good, due to future rewards.\
choosing the best action (being selfi |\
the best rewards. Some actions do not | From our definition of what it means \
beneficial action at the moment, but  | we shouldn't be bad (not good), which\
opportunities.                        | picking the action that maximizes the\
                                      | words, being bad is being always gree\
  Philosophy Textbook                 | important foundation from the course \
                                      | underlying commonalities from differe\
The first argument is from the course <\
action isn't to be greedy, but to be  <\
                                      <\
The previous sections talk about diff <\
this quotation below concludes about  <\
from those different social theories[ <\
page 215 of The Fundamentals of Ethic   page 215 of The Fundamentals of Ethic\
]. I won't go in detail about what ex | ]:\
theories are, but merely look at the  |\
on their shared commonalities:        | The key to understanding them [social\
                                      | however, lies in the idea that the co\
The key to understanding [the social  | rational and self-interested. \
in the idea that contractors are, abo <\
self-interested.                      <\
\
Being self-interested is not the same   Being self-interested is not the same\
Being self-interested is having a str   Being self-interested is having a str\
you are faring in life. Being selfish   you are faring in life. Being selfish\
importance on your own well-being rel   importance on your own well-being rel\
others.                                 others. \
\
While being selfish maximize your own | What it means is that there are futur\
theories have similar commonalities i | a cost of gaining less benefit at-the\
isn't being selfish but rather being  | be self-interested instead of being s\
saying the best action isn't the same | maximizes the reward we get now but a\
                                      | is also seen in the AI textbooks, thr\
This supports the idea that there are | function[footnote:\
good, at a cost of gaining less benef | Also known as an utility function.\
selfish maximizes the present-moment  | ], measuring how much reward was atta\
future rewards.                       | self-interest is to choose the action\
                                      | reward, i.e. the optimal action to ta\
  Artificial Intelligence Textbook    | that maximizes the benefit now, and i\
                                      | [r]epeated experiments show that the \
The second argument is from the AIMA  | converges to the optimal policy\'94[foot\
reinforcement learning, supporting th <\
isn't always best for yourself.       <\
                                      <\
I define some terms for readers in th <\
learning agent has a fixed policy tha <\
whereas an active agent must decide w <\
utility function (also known as rewar <\
the agent's performance. A state encl <\
action. When actions and the search s <\
utility function can be learned by va <\
                                      <\
The experiment setup is to finding an <\
grid. But the grid is non-determinist <\
take may or may not result in the des <\
choses to move left, but the move hap <\
                                      <\
Take it without proof that in this se <\
be extracted by one-step look-ahead t <\
utility. After the 276^\{th\}           <\
 value iteration and learning the     <\
optimal policy at each step, the poli <\
sticks to using that policy, never le <\
other states. We call this agent the  <\
chooses is optimal for each state (pl <\
interestingly, the agent does not lea <\
true optimal policy.                  <\
                                      <\
Repeated experiments show that the gr <\
converges to the optimal policy[footn <\
pg. 839 AIMA 3rd ed.                    pg. 839 AIMA 3rd ed.\
]!                                    | ]. Essentially, the AI textbooks also\
                                      | the philosophy textbook \'96 the idea th\
How can it be that choosing the optim | and self-interested but not selfish. \
suboptimal results? The answer is tha |\
the same as the true environment. An  |   Secondly, why it rewards rational a\
between exploitation (to maximize its |   being good.\
maximize its long-term well-being). P |\
getting stuck in a rut. Pure explorat | In other words, we'll look at how rew\
knowledge is of no use if one never p | agents cooperate with each other and \
practice.                             | understand that when others have good\
                                      | have the trust of others, this opens \
The greedy agent's selfish strategy t | choice of options to choose from. off\
rewarding action doesn't imply the be | collective group than being bad (gree\
out on other opportunities. The optim | happens when everyone is good, when e\
of benefits from the future, through  | when everyone is bad except you, and \
                                      | bad. To the diligent reader, this ind\
  Part I: Conclusion                  | Dilemma. And indeed we will look at N\
                                      | this, we'll look at why behaving badl\
The two textbooks shares the conclusi | form of a collective action problem.\
isn't necessarily one that maximizes  <\
one that factors in the future long t <\
                                      <\
  Part II: Collective benefits        <\
                                      <\
The main idea in this part is it is i <\
because being good benefits the colle <\
Rewards to a group of rational agents <\
                                      <\
I explore the readings from Collectiv <\
the problem as a prisoner's dilemma p <\
case of Nash equilibrium. Then applyi <\
learned about the prisoner's dilemma. <\
\
  Collective Action Problem               Collective Action Problem\
\
In the Collective Action Problem [foo | In the Collective Action Problem read\
p. 366 Constellations Volume 7, Numbe   p. 366 Constellations Volume 7, Numbe\
Irrationality.                          Irrationality. \
http://homes.chass.utoronto.ca/~jheat   http://homes.chass.utoronto.ca/~jheat\
3, 2015                                 3, 2015\
], people behave irrationally because | ], people seem to behave irrationally\
but at a cost to others.              | best action, at a cost to others. Thi\
                                      | Prisoner's Dilemma problem, which we \
The reading suggests that we are lazy | agent perspective. This reading sugge\
to be the one putting in the effort t | individuals who don't want to be the \
is being good. And if everyone is bei | be good, unless everyone is doing goo\
is easier to not be good and reap the | good, then certainly it is going be i\
catch-22 situation.                   | back and reap the fruits of other's l\
                                      | catch-22 problem. \
This problem can be casted to a priso |\
well known case of Nash equilibrium,  | While you can reap rewards by being b\
learnt from prisoner's dilemma. The c | so. Assuming that people are not fool\
                                      | not always rational agents) is quite \
1. Everyone being lazy and you are la | reasonable to assume others learn eve\
  both defect.                        | the whole system collapses. There won\
                                      | to reap. Everyone being good is an un\
2. Everyone being good and you are la | Prisoner's Dilemma. \
  friend cooperates and you defect.   |\
                                      | Thus it is in our self-interest to be\
3. Everyone being good and you are go | good, stable equilibrium and cooperat\
  both cooperate.                     | benefit is greater than individual gr\
                                      | essentially the Nash equilibrium conc\
4. Everyone being lazy and you are go |\
  friend defects and you cooperate.   | We argued these points through analys\
                                      | agent, but the keen reader will notic\
Now I apply results learnt from analy | Equilibrium is a dominate strategy. T\
dilemma.                              | focus of this section is (cleverly) r\
                                      | rational agents as a group. Hence it \
  Prisoner's Dilemma                  | Well why should the rational agent ca\
                                      | agents? The answer to this is that ot\
A key assumption made is the prisoner | rational, meaning they know your stra\
reward or punish their partner and th | advantage of them.\
affect their reputation in the future |\
                                      |   Results.\
This assumption is not realistic in t |\
punishment is death. Similarly, this  | The reader should see that it is in a\
the collective action problem that yo | self-interest to be good because it i\
cooperating. Realistically there woul | above two reasons. First, making bad \
from the collective good on your beha | decreases future expected rewards. Se\
reward, thus it benefits you to coope | group forms a Nash Equilibrium, which\
                                      | not being good destroys the cooperati\
But what if the assumption holds?     | agents and results in overall less re\
                                      |\
Since betraying a partner offers a gr |   Archiving Same Goals Through Just v\
cooperating with him, all purely rati |\
betray the other, and so the only pos | This section explores the issue of di\
prisoners is for them to betray each  | the same goals, but the methods have \
the time element is added to the cons | goodness. \
iterated prisoner's dilemma, then coo |\
rational outcome, as well explained i | Roughly translated, being just transl\
version of prisoner's dilemma, in the | unjust means up to no good. A reading\
chapter [footnote:                    | The Ring of Gyges\'94 by Plato[footnote:\
p. 118 Multi-agent Interactions, in A | \'93The Ring of Gyges\'94 by Plato - Philos\
Systems.                              | http://philosophy.lander.edu/intro/ar\
                                      > June 2, 2015.\
                                      > ], an example of an unjust man vs. a \
                                      > goals. \
                                      >\
                                      > In the reading, it explores a sly, un\
                                      > and just man. Both of whom theoretica\
                                      > goals but through different methods. \
                                      > injustice and deceit can live a life \
                                      > achieving the same goals through just\
                                      > deceit. Intuition tells us that archi\
                                      > through deceitful means is certainly \
                                      > through honorable means. And indeed P\
                                      > course textbook summarizes it well[fo\
                                      > page 108 of The Fundamentals of Ethic\
].                                      ].\
\
The game of the prisoner's dilemma is | Certainly many immoral people are dee\
Each play is referred to as a round.  | But others are able to sleep well at \
that each agent can see what the oppo | well done (assassination, theft, betr\
round: player i                       | within a network of like-minded assoc\
 can see whether j                    | sometimes get away with it, having a \
 defected or not, and j               | and never regret the harm they have c\
 can                                  |\
see whether i                         | Certainly based on pure accomplishmen\
 defected or not. Now, for the sake o | same, since both can accomplish their\
assume that the agents will continue  | or unjust means. But the primary diff\
every round will be followed by anoth | chance for the dishonest man to get c\
assumptions, what is the rational thi | penalized. And sure, certainly the ba\
                                      | with it. A major assumption here is t\
If you know that you will be meeting  | it. \
rounds, the incentive to defect appea |\
diminished, for two reasons.          | Let's break the argument into two pie\
                                      | sometimes in lying, and suppose he ne\
Reason 1: If you defect now, your opp |\
defecting. Punishment is not possible |   He might fail at deceit.\
dilemma.                              |\
                                      | This is the more realistic argument. \
Reason 2: If you 'test the water' by  | underlying assumption is unattainable\
receive the sucker's payoff on the fi | entire life from the moment they were\
are playing the game indefinitely, th | outrageous assumption, we might as we\
util) can be 'amortized' over the fut | everything he could ever want in life\
                                      | goal but taking the unjust method to \
When taken into the context of an inf | more risk, but at a chance to gain hi\
long) run, then the loss of a single  |\
represent a small percentage of the o | But here is the kicker: there is no d\
if you play the prisoner's dilemma ga | had assumed the two man achieved the \
cooperation is a rational outcome.    | means. Then obviously it makes no sen\
                                      | All the deceitful man has done is sav\
The summary is that selfish rationali | Which may be valuable, but at a heavy\
possible result. Maximizing individua |\
possible reward. There are more benef |   He never fails at deceit.\
collectively, if they both cooperated |\
that being good benefits more as a wh | Even if we make the (unlikely) assump\
individually.                         | capable of maintaining his composure \
                                      | out of character, it still doesn't ma\
Interestingly, humans display a syste | riskier path as it leads to the same \
cooperative behavior in this, which g | save some time and effort. Seems that\
rationales[footnote:                  | difficult to accomplish (if not impos\
Tversky, Amos; Shafir, Eldar (2004).  | layman. Sure, there may exist those o\
similarity: selected writings. (PDF). | enough to carry this out, but they mo\
Technology Press. Retrieved July 26,  | argument. It is likely the assumption\
]. It makes sense because the idea of | cannot be proved.\
into play, because the assumption tha |\
to reward or punish their partner is  |   Results.\
                                      |\
  Part II: Conclusion                 | A lot of the arguments explored here \
                                      | solid in logic analysis as from the r\
A rational agent's self-interest is t | But nevertheless, the fundamental arg\
rewarding. Rational agents in a group | show that it's much easier to maintai\
Nash equilibrium, so being good means | composure than a deceitful and unjust\
more rewards. In the face of punishme | it difficult to maintain a deceitful \
selfish decisions decreases future ex | risks. Consequently, if we act just, \
other agents are less inclined to coo | self-interest to be good.\
defecting agent.                      <\
\
  Conclusion                              Conclusion\
\
I conclude that it is in our self-int | We've now explored this problem from \
analyzed in the above two parts. Part | rational agent would act, and their p\
reaps more long term rewards, and Par | in a rational agent's self-interest t\
benefits more as a whole.             | rewarding, because making bad (not go\
                                      | decreases future expected rewards, an\
                                      | the cooperation with other rational a\
                                      | less reward. \
                                      |\
                                      | And we've followed up on what do we d\
                                      | different methods to solving a proble\
                                      | good and some not. It seems that the \
                                      | reasonably invalid, and even if it wa\
References                            | be gained.\
                                      <\
[1]                                   <\
                                      <\
[2]                                   <\
                                      <\
[3]                                   <\
                                      <\
[4]                                   <\
\
[5]                                   | The common result from the analysis s\
                                      > optimal choice. \
\
}