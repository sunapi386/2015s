CS 486 Assignment 3
====================
Jason Sun (#20387090)

Question 2: Naive Bayes Model
-----------------------------

### Code printout

Naive bayes main function.

    % Jason Sun (#20387090)
    loadScript
    [log_doc1, log_doc2, log_likelihood] = naive_bayes_net(trainDataSparse, trainLabel);
    % list the N most discrimative word features
    [sortedVals, sortedIdx] = sort(log_likelihood(:), 'descend');
    N = 10;
    maxIdxs = sortedIdx(1:N);
    disp(['Top ', num2str(N), ' most discriminative words:']);
    for i = 1:length(maxIdxs)
       disp([num2str(log_likelihood(maxIdxs(i))), words(maxIdxs(i))]);
    end
    % return the % of correctly classified articles
    % each document (represented by a row in sparse table), multiply
    % each word (a cell within a row) with the log probability in the doc
    % for the training set
    trainClassDoc1 = (trainDataSparse * log_doc1);
    trainClassDoc2 = (trainDataSparse * log_doc2);
    % then classify which document it is by whichever is larger
    trainClassification = (trainClassDoc1 < trainClassDoc2) + 1;
    trainCorrects = sum(trainClassification == trainLabel) / length(trainLabel);
    disp(['Classified ', num2str(trainCorrects * 100), '% correct for training data.']);
    % for the testing set
    [test_log_doc1, test_log_doc2, ~] = naive_bayes_net(trainDataSparse, trainLabel);
    testClassDoc1 = (testDataSparse * test_log_doc1);
    testClassDoc2 = (testDataSparse * test_log_doc2);
    % then classify which document it is by whichever is larger
    testClassification = (testClassDoc1 < testClassDoc2) + 1;
    testCorrects = sum(testClassification == testLabel) / length(testLabel);
    disp(['Classified ', num2str(testCorrects * 100), '% correct for testing data.']);

  Naive bayes net calculation.

    function [log_doc1, log_doc2, log_likelihood] = naive_bayes_net(sparse_data, label)
    % returns the log likelihood, section 20.2 AIMA 3rd ed.
    % split training set into two documents
    doc1 = [];
    doc2 = [];
    for i = 1:length(label)
       if label(i) == 1
           doc1 = [doc1; sparse_data(i,:)];
       else
           doc2 = [doc2; sparse_data(i,:)];
       end
    end
    % calc Pr(word|label1) and Pr(word|label2)
    pr_word_doc1 = zeros(length(sparse_data(1,:)),1);
    pr_word_doc2 = zeros(length(sparse_data(1,:)),1);
    % start count at 1 for Laplace smoothing
    for word = 1:length(sparse_data(1,:))
       count = 1;
       for i = 1:size(doc1, 1)
          count = count + doc1(i,word);
       end
       pr_word_doc1(word) = count / size(doc1, 1);
       count = 1;
       for i = 1:size(doc2, 1)
          count = count + doc2(i,word);
       end
       pr_word_doc2(word) = count / size(doc2, 1);
    end
    log_doc1 = log(pr_word_doc1);
    log_doc2 = log(pr_word_doc2);
    log_likelihood = abs(log_doc1 - log_doc2);
    end


### Results

    Top 10 most discriminative words:
    '4.4242'    'graphics'
    '3.9752'    'atheism'
    '3.9286'    'religion'
    '3.8545'    'keith'
    '3.8545'    'moral'
    '3.8545'    'evidence'
    '3.8286'    'atheists'
    '3.7837'    'god'
    '3.7463'    'bible'
    '3.7173'    'christian'

These seem like good word features. I can classify each of these words into
the athesim or the graphics category. The only word that puzzles me is 'keith'.

### Training and test accuracy

Classified 77.2856% correct for training data.
Classified 0.72136% correct for testing data.

### Independence assumtion

The assumption that word features are independent is not a reasonable assumption.
Same or similar words to what was used before are more likely to show up again in
the future because when writing involves context.

For example, translation of documents to a different language is a difficult
process, because the choice of words depends on context, especially in cases
where multiple words have similar meanings.

### Improvements to the naive Bayes model

The improvement is difficult without changing too much the modeling method.
Introducing a dependency of words to the model would change it from being a
naive Bayes model. Nevertheless, few improvements can be made, as discussed
in an MIT paper.
[http://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf]

The improvements can be generalized to be:

1.  Analysing term frequency
2.  Analysing document frequency
3.  Analysing document length frequency


### Decision tree model vs naive Bayes model

The decision tree modeling approach works better because it can model
importance of words based in a hierarchical fashion. If a word is close to the
root node, then it will have a greater effect on the decision. For instance,
the keyword 'graphics' showed up, then we probably do not need to look at any
more words in the document before making a decision. This means word ordering
matters, so a document containing just "god graphics" is classified as atheism
topic, which is differently than "graphics god", classified as graphics topic.
Whereas the naive Bayes model gives every word an equal weighting, so that
"god graphics" is the same category as "graphics god", both being in the
graphics topic.


